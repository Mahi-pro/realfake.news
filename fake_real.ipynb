from google.colab import files
uploaded = files.upload()
No file chosen Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Saving Fake.csv to Fake.csv
Saving True.csv to True.csv
!pip install transformers datasets torch scikit-learn
import pandas as pd
from sklearn.model_selection import train_test_split

# Load datasets
fake = pd.read_csv("Fake.csv")
real = pd.read_csv("True.csv")

# Add labels: 0 = fake, 1 = real
fake['label'] = 0
real['label'] = 1

# Combine and shuffle
df = pd.concat([fake[['text', 'label']], real[['text', 'label']]])
df = df.dropna()
df = df.sample(frac=1).reset_index(drop=True)

# Train-test split
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42
)
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)
import torch

class NewsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),
            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),
            'labels': torch.tensor(self.labels[idx])
        }

train_dataset = NewsDataset(train_encodings, train_labels)
test_dataset = NewsDataset(test_encodings, test_labels)
import os
os.environ["WANDB_DISABLED"] = "true"
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
import os

# Disable WandB tracking
os.environ["WANDB_DISABLED"] = "true"

# Load pre-trained BERT model with classification head
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Training configuration
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    logging_strategy='epoch',
    save_strategy='epoch',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    load_best_model_at_end=True,
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Start training
trainer.train()
trainer.train(resume_from_checkpoint=True)
metrics = trainer.evaluate()
print(metrics)
predictions = trainer.predict(test_dataset)
preds = predictions.predictions.argmax(-1)

# Optionally print or compare with actual labels
print(preds)
model.save_pretrained("./fake_news_model")
tokenizer.save_pretrained("./fake_news_model")
from transformers import BertForSequenceClassification, BertTokenizer

model = BertForSequenceClassification.from_pretrained('./fake_news_model')
tokenizer = BertTokenizer.from_pretrained('./fake_news_model')
from transformers import pipeline

classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

result = classifier("Breaking: NASA discovers water on Mars!")
print(result)
model.save_pretrained("bert-fake-news-model")
tokenizer.save_pretrained("bert-fake-news-model")
!zip -r bert-fake-news-model.zip bert-fake-news-model
from google.colab import files
files.download("bert-fake-news-model.zip")
